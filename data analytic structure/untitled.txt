Список задач для аналитика данных на год в компании, занимающейся мониторингом оборудования. Они покрывают работу с данными, автоматизацию процессов и создание аналитических инструментов.

---

### **1-3 месяц** (Вхождение в проект, изучение данных, первые задачи)
✅ Разобраться со структурой данных в PostgreSQL, изучить схему БД, источники данных о работе оборудования.  
✅ Настроить DAG в Apache Airflow для автоматического сбора данных с датчиков.  
✅ Оптимизировать SQL-запросы для ускорения получения данных о работе оборудования.  
✅ Подготовить первый аналитический отчет о текущем состоянии оборудования для внутренних отделов.  
✅ Настроить интеграцию с Git для версионирования SQL-скриптов и Python-кода.  

---

### **4-6 месяц** (Оптимизация процессов, автоматизация отчетности)
✅ Разработать автоматизированный отчет по аварийным событиям оборудования с расчетом средних показателей отказов.  
✅ Настроить DAG в Airflow для ежедневного обновления отчетов о работоспособности оборудования.  
✅ Разработать SQL-запросы и Python-скрипты для расчета степени износа оборудования на основе показателей работы.  
✅ Визуализировать данные по отказам и прогнозируемым поломкам в виде дашбордов (Tableau / Python / Grafana).  
✅ Создать систему алертов: если оборудование приближается к критическому уровню износа — отправлять уведомление в Telegram/Slack.  

---

### **7-9 месяц** (Продвинутый анализ данных, предсказательное моделирование)
✅ Разработать модель прогнозирования отказов на основе исторических данных (Python, ML-модель или продвинутые статистические методы).  
✅ Оптимизировать SQL-запросы и DAG в Airflow для ускорения обработки данных.  
✅ Разработать и внедрить новую витрину данных с агрегированной информацией о работе оборудования для топ-менеджмента.  
✅ Подготовить отчет о загруженности оборудования и его эффективном использовании.  
✅ Добавить мониторинг аномалий в данных: если показания резко отклоняются от нормы, система автоматически формирует предупреждение.  

---

### **10-12 месяц** (Поддержка и развитие аналитической системы)
✅ Улучшить алгоритмы расчета эффективности оборудования (включая энергопотребление, время простоя и др.).  
✅ Провести аудит данных в PostgreSQL, выявить возможные дубли и несоответствия.  
✅ Разработать новые KPI и дашборды для внутренних отделов (технический отдел, финансовый департамент, производство).  
✅ Автоматизировать процесс загрузки новых данных в систему, используя API или парсинг логов оборудования.  
✅ Настроить механизм обратной связи с пользователями дашбордов: какие метрики важны, что нужно улучшить.  

---

Вот пошаговое руководство по настройке DAG в Apache Airflow для автоматического сбора данных с датчиков и записи их в PostgreSQL.  

---

## **1. Установка Apache Airflow (если не установлен)**
Если Airflow ещё не установлен, развернём его локально с использованием `pip`:
```bash
pip install apache-airflow
```
Подключаемся к существующей базе данных и инициализируем Airflow:
```bash
psql -U postgres -h localhost

Если подключение работает, создадим базу для Airflow:

CREATE DATABASE airflow_db;
CREATE USER airflow_user WITH ENCRYPTED PASSWORD 'strongpassword';
GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;

```
Настрой Airflow для использования PostgreSQL

Открой airflow.cfg (по умолчанию в ~/airflow/airflow.cfg) и найди параметр:

sql_alchemy_conn = sqlite:////path/to/airflow.db
Замени на подключение к PostgreSQL:

sql_alchemy_conn = postgresql+psycopg2://airflow_user:strongpassword@localhost/airflow_db

```
Переинициализируй базу данных
airflow db init
Теперь Airflow будет использовать PostgreSQL вместо SQLite

Создаём пользователя для входа в веб-интерфейс:
```bash
airflow users create \
    --username admin \
    --password admin \
    --firstname John \
    --lastname Doe \
    --role Admin \
    --email admin@example.com
```
Запускаем Airflow Scheduler и Web Server:
```bash
airflow scheduler &
airflow webserver -p 9090 &
```
Теперь интерфейс доступен по адресу: [http://localhost:8080](http://localhost:9090)

---

## **2. Создание подключения к PostgreSQL**
Airflow использует **Connections** для работы с внешними БД. Настроим подключение через UI:
1. Открываем [http://localhost:8080](http://localhost:8080).
2. Переходим в **Admin → Connections**.
3. Нажимаем **+ Add Connection**.
4. Заполняем данные:
   - **Connection Id**: `postgres_default`
   - **Conn Type**: `Postgres`
   - **Host**: `your_postgres_host`
   - **Schema**: `your_database`
   - **Login**: `your_username`
   - **Password**: `your_password`
   - **Port**: `5432`
5. Сохраняем.

---

## **3. Создание DAG для сбора данных с датчиков**
Теперь создадим **DAG**, который будет:
- Запрашивать данные с датчиков через API.
- Записывать их в PostgreSQL.

### **3.1 Создаём DAG**
Создадим файл **`sensor_data_dag.py`** в директории `dags/`:
```python
from airflow import DAG
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import requests
import json

# Параметры DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2024, 3, 31),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# Функция для получения данных с датчиков через API
def fetch_sensor_data():
    url = "https://api.example.com/sensor-data"  # URL API датчиков
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return data
    else:
        raise Exception(f"Ошибка запроса API: {response.status_code}")

# Функция для записи данных в PostgreSQL
def insert_sensor_data(**kwargs):
    from airflow.providers.postgres.hooks.postgres import PostgresHook
    
    pg_hook = PostgresHook(postgres_conn_id="postgres_default")
    data = kwargs['ti'].xcom_pull(task_ids='fetch_data')

    if not data:
        raise ValueError("Нет данных для записи")

    insert_query = """
        INSERT INTO sensor_data (sensor_id, value, timestamp)
        VALUES (%s, %s, %s);
    """

    with pg_hook.get_conn() as conn:
        with conn.cursor() as cursor:
            for entry in data:
                cursor.execute(insert_query, (entry["sensor_id"], entry["value"], entry["timestamp"]))
        conn.commit()

# Определяем DAG
with DAG(
    "sensor_data_dag",
    default_args=default_args,
    description="Сбор данных с датчиков и запись в PostgreSQL",
    schedule_interval=timedelta(minutes=30),  # Запуск каждые 30 минут
    catchup=False,
) as dag:

    # Шаг 1: Получаем данные с API
    fetch_data = PythonOperator(
        task_id="fetch_data",
        python_callable=fetch_sensor_data,
    )

    # Шаг 2: Записываем в PostgreSQL
    insert_data = PythonOperator(
        task_id="insert_data",
        python_callable=insert_sensor_data,
        provide_context=True,
    )

    # Указываем порядок выполнения
    fetch_data >> insert_data
```

---

## **4. Настройка базы данных (PostgreSQL)**
Выполним SQL-запрос для создания таблицы:
```sql
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id VARCHAR(50),
    value FLOAT,
    timestamp TIMESTAMP
);
```

---

## **5. Запуск DAG**
Теперь активируем наш DAG:
1. Перезапускаем Airflow Scheduler:
   ```bash
   airflow scheduler
   ```
2. Открываем [http://localhost:8080](http://localhost:8080).
3. Заходим в **DAGs** → **sensor_data_dag**.
4. Нажимаем **Trigger DAG** для ручного запуска.

---

## **6. Проверка данных**
После успешного выполнения DAG проверим данные в PostgreSQL:
```sql
SELECT * FROM sensor_data ORDER BY timestamp DESC LIMIT 10;
```

---

## **7. Дополнительные улучшения**
1. **Добавить алерты в случае сбоя**: можно использовать EmailOperator или Telegram API.
2. **Добавить логирование**: записывать успешные и неудачные записи в отдельную таблицу.
3. **Обработку исключений**: например, проверять, не повторяются ли записи.

---

### **Готово! Теперь Airflow автоматически собирает данные с датчиков и записывает их в PostgreSQL 🚀**



Подробно пропишем с 6 шага

### **🔧 Настройка DAG в Apache Airflow для автоматического сбора данных с датчиков**  
💡 **Цель**:  
- Подписаться на MQTT-топик (где Wiren Board публикует данные с датчиков).  
- Получить JSON-данные.  
- Записать их в PostgreSQL.  
- Использовать DAG в Airflow для автоматического выполнения задачи.  

---

## **🔹 1. Подготовка окружения**
Перед настройкой DAG в Airflow, убедимся, что все компоненты установлены и работают.

### **🛠 Требуемые компоненты:**
✅ **Apache Airflow** – управление задачами (ETL, сбор данных).  
✅ **PostgreSQL** – база данных для хранения данных с датчиков.  
✅ **Mosquitto MQTT** – брокер сообщений (для получения данных от Wiren Board).  
✅ **Paho-MQTT (Python библиотека)** – для подписки на MQTT-топик.  
✅ **psycopg2 (Python библиотека)** – для записи данных в PostgreSQL.  

### **📌 Установим зависимости**
```bash
pip install apache-airflow psycopg2 paho-mqtt
```

---

## **🔹 2. Настройка PostgreSQL для хранения данных с датчиков**
Создадим таблицу `sensor_data` в PostgreSQL:  
```sql
CREATE TABLE sensor_data (
    id SERIAL PRIMARY KEY,
    sensor_id TEXT NOT NULL,
    value NUMERIC NOT NULL,
    unit TEXT NOT NULL,
    timestamp TIMESTAMP NOT NULL
);
```
💾 **Эта таблица будет хранить данные, которые мы получим от MQTT.**

---

## **🔹 3. Подключение Airflow к PostgreSQL**
Airflow должен "знать", куда записывать данные.  
Настроим **Connection** в Airflow UI:

1. Открываем **Airflow Web UI**  
   ```bash
   airflow webserver -p 8080
   ```
2. Заходим в **Admin → Connections**.  
3. Нажимаем **+ Add Connection**.  
4. Заполняем поля:  
   - **Conn Id**: `postgres_sensor_db`  
   - **Conn Type**: `Postgres`  
   - **Host**: `localhost`  
   - **Schema**: `sensors`  
   - **Login**: `admin`  
   - **Password**: `secret`  
   - **Port**: `5432`  
5. **Сохраняем**.

Теперь Airflow сможет работать с PostgreSQL. ✅  

---

## **🔹 4. Создание Python-скрипта для получения данных с MQTT**
Этот скрипт будет подписываться на MQTT, получать JSON-данные и записывать их в PostgreSQL.

📌 **Создадим `mqtt_to_postgres.py` в папке DAGs**  
📂 **Путь**: `~/airflow/dags/mqtt_to_postgres.py`

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
import paho.mqtt.client as mqtt
import json
from datetime import datetime, timedelta

# Параметры DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2024, 3, 31),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

dag = DAG(
    "mqtt_to_postgres",
    default_args=default_args,
    schedule_interval="*/5 * * * *",  # Запуск каждые 5 минут
    catchup=False,
)

# Функция обработки сообщений
def on_message(client, userdata, message):
    data = json.loads(message.payload.decode("utf-8"))
    pg_hook = PostgresHook(postgres_conn_id="postgres_sensor_db")
    conn = pg_hook.get_conn()
    cursor = conn.cursor()
    
    insert_query = """
        INSERT INTO sensor_data (sensor_id, value, unit, timestamp)
        VALUES (%s, %s, %s, %s)
    """
    cursor.execute(insert_query, (data["sensor_id"], data["value"], data["unit"], data["timestamp"]))
    conn.commit()

# Функция запуска MQTT-клиента
def run_mqtt():
    client = mqtt.Client()
    client.connect("mqtt_broker_address")  # Укажи адрес брокера (например, localhost)
    client.subscribe("sensors/temperature")
    client.on_message = on_message
    client.loop_start()

# Airflow задача
mqtt_task = PythonOperator(
    task_id="mqtt_listener",
    python_callable=run_mqtt,
    dag=dag,
)

mqtt_task
```
---

## **🔹 5. Добавление DAG в Airflow**
1️⃣ **Скопировать `mqtt_to_postgres.py` в папку DAGs Airflow**  
```bash
mv mqtt_to_postgres.py ~/airflow/dags/
```
2️⃣ **Перезапустить Airflow Scheduler**  
```bash
airflow scheduler &
```
3️⃣ **Зайти в Airflow UI и активировать DAG**  
- Открыть `http://localhost:8080`
- Найти **mqtt_to_postgres**
- Включить (тумблер в `ON`)
- Проверить выполнение задач в **Graph View**

---

## **🔹 6. Проверка работы**
1️⃣ **Посмотрим логи DAG**  
```bash
airflow tasks test mqtt_to_postgres mqtt_listener 2024-03-31
```
✅ Должен появиться вывод, подтверждающий успешное подключение к MQTT и вставку данных в PostgreSQL.

2️⃣ **Проверим PostgreSQL**  
```bash
psql -U admin -d sensors -h localhost -W
```
```sql
SELECT * FROM sensor_data ORDER BY timestamp DESC LIMIT 10;
```
✅ Должны появиться новые строки с данными от датчиков.

---

## **🔹 7. Дальнейшая обработка данных**
Теперь, когда данные записаны в PostgreSQL, можно:
- 📊 **Анализировать их с помощью Python (Pandas, SQLAlchemy)**
- 📉 **Отображать в Grafana или BI-инструментах**
- 📢 **Настроить алерты (если температура выше 70°C → оповещение)**

---

### **🎯 Итог**
📌 **Мы настроили Airflow DAG**, который:
1. **Запускается каждые 5 минут**.
2. **Подписывается на MQTT-топик `sensors/temperature`**.
3. **Принимает JSON-данные от Wiren Board**.
4. **Записывает данные в PostgreSQL**.
5. **Автоматизирует процесс сбора данных с датчиков**.

Теперь данные о температуре с датчиков **автоматически собираются, обрабатываются и сохраняются** в БД! 🚀
